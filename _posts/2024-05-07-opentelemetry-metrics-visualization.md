---
layout: post
title:  "The ABCs of semantic search in OpenSearch: Architectures, benchmarks, and combination strategies"
authors:
 - kschnitter
date: 2024-05-03
categories:
 - technical-post
meta_keywords: 
meta_description: 
excerpt: OpenSearch provides OpenTelemetry ingestion by using DataPrepper as ingestion tool. We explore, how to analyze and visualize metrics indexed by that route.
has_math: true
has_science_table: true
---

OpenSearch is growing into a full observability platform supporting logs, metrics, and traces.
Using DataPrepper, it is capable of ingesting these three signal types in OpenTelemetry format.
_Logs_ are supported well by the search index and other core features from OpenSearch and OpenSearch Dashboards.
_Traces_ have their own section within the Observability plugin providing flame graphs, service maps and much more.
The _metrics_ support in OpenSearch is geared towards the integration of an external Prometheus data source.
Analyzing metrics ingested with DataPrepper requires custom built visualizations.
This blog post is all about these visualizations and the underlying data model.
It is built on the experience in [SAP Cloud Logging service](https://discovery-center.cloud.sap/serviceCatalog/cloud-logging), where I created per-defined dashboards for the customers.

### Outline

- [Example Setup](#example-setup)
- [OpenTelemetry Data Model](#opentelemetry-metrics-data-model)
- [Discover and Search the Data](#discover-inspecting-the-raw-data-and-saved-searches)
- [Visualizations with TSVB](#visualizations-with-tsvb)
- [Standard Visualizations](#standard-visualizations)
- [Vega](#outlook-vega)

### Example Setup

Let us start with a common and easy to reproduce scenario for metrics generation.
In this case we assume a running Kubernetes cluster for which we want to analyse the K8s container metrics.
These metrics can be generated by the [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/) using the [Kubelet Stats Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/README.md).
See the linked documentation on how to best setup the receiver in your cluster.
There are several options for authentication to the K8s endpoints.

To ship data to DataPrepper, all that is needed is an `otlp` exporter using the gRPC protocol:

```yaml
exporters:
  otlp:
    endpoint: "<your-DataPrepper-url>"
    tls:
      cert_file: "./cert.pem"
      key_file: "./key.pem"
```

This example uses mTLS, but you can also use other authentication options.
The corresponding DataPrepper configuration looks something like this:

```yaml
metrics-pipeline:
    source:
    otel_metrics_source:
        ssl: true
        sslKeyCertChainFile: "./cert.pem"
        sslKeyFile: "./key.pem"
    processor:
    - otel_metrics:
    sink:
    - opensearch:
        hosts: [ "<your-OpenSearch-url>" ]
        username: "otelWriter"
        password: "<your-password>"
        index: metrics-otel-v1-%{yyyy.MM.dd}
```

This example uses Basic Auth for OpenSearch.
It requires the `otelWriter` backend role with sufficient permissions for DataPrepper.
This is similar to trace ingestion.

The complete data flow of this setup is shown in the following diagram:

![Data Flow of OTel Ingestion](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/otel-ingestion-architecture.drawio.png){:class="img-centered"}

When this setup is complete, you will have a lot of metrics in OpenSearch to analyze.
There is a complete [documentation](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kubeletstatsreceiver/documentation.md) of the metrics and metadata emitted by the Kubelet Stats Receiver.

> Hint: If you are a service provider for development teams, this setup can be automated and transparent to your users.
Within SAP BTP, this is achieved by the [Kyma Telemetry module](https://kyma-project.io/#/telemetry-manager/user/README) in connection with [SAP Cloud Logging](https://discovery-center.cloud.sap/serviceCatalog/cloud-logging).
It is this solution, that forms the foundation of this article.

### OpenTelemetry Metrics Data Model

OpenTelemetry uses a well documented [Metrics Data Model](https://opentelemetry.io/docs/specs/otel/metrics/data-model/).
The linked article explains all the details of the various supported metric types.
We will do a less thorough approach and inspect example points to collect the necessary information.
With our example setup, we can find data points similar to this (shortened) example:

```json
{
  "_source": {
    "name": "container.cpu.time",
    "description": "Total cumulative CPU time (sum of all cores) spent by the container/pod/node since its creation",
    "value": 120.353905,
    "flags": 0,
    "unit": "s",
    "startTime": "2024-04-10T06:49:56Z",
    "time": "2024-05-03T07:50:22.197391185Z",
    "kind": "SUM",
    "aggregationTemporality": "AGGREGATION_TEMPORALITY_CUMULATIVE",
    "isMonotonic": true,
    "serviceName": "otel-emailservice",
    "instrumentationScope.name": "otelcol/kubeletstatsreceiver",
    "instrumentationScope.version": "0.97.0",
    "resource.attributes.k8s@namespace@name": "example",
    "resource.attributes.k8s@deployment@name": "otel-emailservice",
    "resource.attributes.k8s@pod@name": "otel-emailservice-7795877686-r2lrt",
    "resource.attributes.k8s@container@name": "emailservice",
    "resource.attributes.service@name": "otel-emailservice"
  }
}
```

We can see, that OpenTelemetry separates metrics by the `name`.
Each point contains the generic `value` and `time`, which form the basic time series for the visualizations.
This schema is a at first glance suboptimal for OpenSearch, where having a field with key `name` and value with the numerical value is easier accessible.
For our example, this would look like `{ "container.cpu.time": 120.353905}`.
However, if we ingested a lot of different metrics, we would reach the field limit of 1000 per index pretty fast.

There is additional information about the kind, that introduces semantics to the time series.
In the example we deal with a "SUM" (a counter), which is monotonic.
The "AGGREGATION_TEMPORALITY_CUMULATIVE" tells us, that the `value` will contain the current count started at `startTime`.
The alternative "AGGREGATION_TEMPORALITY_DELTA" would contain only the change encountered between `startTime` and `time` with non-overlapping intervals.
We need to take these semantics into account, e.g., if we want to visualize the rate of changes to our metrics.

Finally, there are additional metadata fields, we can use to slice our data by different dimensions.
There are three different groups:

- _resource attributes_ describe the metric source. You can see the K8s coordinates in the example.
- _metric attributes_ describe particular aspects of the metric. There is no such attribute in our example, but you could imagine memory pools or similar for this.
- _instrumentation scope_ describes the method of metrics acquisition. In our case, this is the Kubelet Stats Receiver of the OpenTelemetry Collector.

Note, that the attributes contain a lot of "@" characters.
In the [OpenTelemetry semantic conventions](https://github.com/open-telemetry/semantic-conventions), that specify attributes and metric names, there are dots "." in that places.
Since OpenSearch treats those dots as nested fields, DataPrepper applies this . -> @ mapping to avoid conflicts.

> Hint: If you want to manipulate OpenTelemetry data with PPL in OpenSearch, you need to use backticks around field names containing the @ symbol.

Now we have everything we need to create our first visualizations: We need to access the `value` over `time` selected by `name`. Additional dimensions come from the attributes.
  
### Discover: Inspecting the Raw Data and Saved Searches

Now we know, what [data model](#opentelemetry-metrics-data-model) to expect, we can investigate the metrics using the Discover plugin.
The following screenshot shows metrics for the [OpenTelemetry Demo](https://github.com/open-telemetry/opentelemetry-demo) collected with out [example setup](#example-setup).

![Data Flow of OTel Ingestion](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/discover_otel-metrics_general.png){:class="img-centered"}

Note the filtering by instrumentation scope and K8s namespace.

We can use the Discover plugin to develop DQL expressions for data filtering, e.g., `name:k8s.pod.cpu.time`, or create saved searches.
Both will be used for visualizations.

One of the main benefits of Discover is the easy access to metrics attributes.
The plugin provides discovery of present attributes and their potential values.

### Visualizations with TSVB

TSVB offers a class of visualizations specialized on time-series data.
This works very well for the metrics we want to analyze.
We start at the visualize plugin.

![Visualize Overview](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_overview.png){:class="img-centered"}

We create a new visualization and choose TSVB.

![New TSVB Visualization](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_create_tsvb.png){:class="img-centered"}

As a first step, we navigate to the "Panel options" below the graph.

![TSVB Panel Options](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_panel-options.png){:class="img-centered"}

We select:
- our example index pattern "metrics-otel-v1-*"
- the time field `time` from the DataPrepper/OpenTelemetry data model
- an interval of at least 1 minute
- the panel filter DQL expression `name:k8s.pod.cpu.time` we explored with [Discover](#discover-inspecting-the-raw-data-and-saved-searches)

The index pattern might be different in your configuration.
Use the same pattern as in [Discover](#discover-inspecting-the-raw-data-and-saved-searches).

The minimum interval is important for shorter search intervals.
TSVB will calculate the interval length automatically and only connect points between adjacent intervals.
If there are intervals without values, the dots will not be connected.

Choosing the panel filter `name:k8s.pod.cpu.time` filters the metrics for the Pod CPU time.
This is the metric we want to visualize first.

We now switch back to the "Data" tab.

![TSVB Data Plain](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_data-max-value.png){:class="img-centered"}

Here we configure the data, we want to visualize.
We start simple and select aggregation "Max" of field "value".
This aligns well with the cumulative aggregation temporality.
In each interval the maximum (last) cpu time is placed as a dot in our diagram.
We group by a "Terms" aggregation on the K8s pod name to get the cpu times per pod.
To avoid over-crowding of our graph, we select Top 3 ordered by "Max of value" in descending direction.
This provides us with a time-line of cpu time by the top 3 pods.

If we were visualizing a metric of kind gauge, this would be it.
However, with our sum, there is more to be done.
The graph looks fine, but the arbitrary `startTime` differing between the pods makes the values hard to compare.
In the next step we calculate the rate of our sum.

![TSVB Data Positive Rate](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_data-positive-rate.png){:class="img-centered"}

We extend the aggregations by a derivative by unit 1s.
This calculates the time share spent by the cpu on that pod.
We add another aggregation to filter for positive values only.
This suppresses artifacts from pod restarts.

Note, that there is a short form for this by replacing the original "Max" aggregation by "Positive Rate".
However, this comes at a cost to the ordering.
On the bottom line of the screenshot you can see, that we can only sort by the "max of value".
This is the best we can do.
If we used "Positive Rate", even that option would vanish.

We can improve the graph by choosing percent as Data Formatter under the Options tab.
This provides a nice view on the CPU utilization history by pod. 

![TSVB Data Options](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_data-options.png){:class="img-centered"}

We can re-aggregate the data of this timeline by a cumulative sum.
This yields the cpu time spent in the selected interval.
Let us also switch the graph type to "Top N".

![TSVB Data Cumulative](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/tsvb_data-cumulative.png){:class="img-centered"}

Now we have an ordered bar chart of our top 10 pods (we need to adjust the number of options in the group by).

We have now created two different visualizations of pod cpu utilization.
We can save our visualizations at any point in time and add them to dashboards, that combine multiple visualizations.
TSVB allows to configure, whether global filters should be respected or not.
This allows to embed the visualizations into larger scenarios sharing filters.
There are much more features within TSVB, that allow more elaborate graphs.
Highlights of TSVB are the relatively easy entry to time series visualization and the options to adjust the graph, especially the data formatter.

### Standard Visualizations

We will now have a look on the standard visualizations offered by OpenSearch Dashboards.
Compared to TSVB, these visualizations are a little more elaborate to build.
However, they allow the creation of filters, that can be applied to the entire dashboard, not only the current visualization.
This makes the ideal as selectors.

#### A Data Table for K8s Namespaces

We start with a data table showing us the number of pods and containers by namespace.
For this, we create a new visualization and choose Data Table.

![New Data Table Visualization](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_create_data-table.png){:class="img-centered"}

We are asked to select the index or saved search as data source.
For our table, we can use the entire OpenTelemetry metrics index.
Let us have a look at the finished table and then walk through the steps, that generate it.

![K8s Pods and Containers per Namespace](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_unique-pod-names.png){:class="img-centered"}

The table lists in each line the number of pods and containers.
Note, that in the example the "kyma-system" namespace consists of 23 pods all running just one Istio container.
This explains the lower number of unique containers compared to the number of pods.

On the right, we can see, how the number of pods is calculated: It is the unique count of values in field `resource.attributes.k8s@pod@name`, which contains the pod name.
We could have used the pod id as well, but pod names contain a unique suffix, so this is semantically identical.
Note, that the pod name is filled in by the Kubelet Stats Receiver as resource attribute.

The count of containers can be added by clicking on the plus button with a very similar configuration.

![K8s Unique Container Count](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_unique-container-names.png){:class="img-centered"}

Note, that we provide custom labels to have nice table headings.
Any changes to our configuration are not applied automatically.
Use the "Update" button on the bottom right to sync the visualization.

We still need to provide separate lines per namespace.
This is achieved with a bucket aggregation.

![K8s Namespace Terms Aggregation](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_namespace-grouping.png){:class="img-centered"}

We choose "split rows" and a "Terms" aggregation for field `resource.attributes.k8s@namespace@name`.
We order our table by the number of pods.
We could also choose the number of containers or any other metrics we defined above.
There is also an option for a completely separate metric.

We can also select the number of entries, we want to retrieve maximally.
The "Options" tab allows us to paginate this result by specifying the number of rows per page.
There we can also select a "total" function.
But in our case, only summing the number of pods would provide a correct number.
The container count might be double counting containers, that appear in different namespaces.
This is why this option was left unchecked.

Back in the "Data" tab, we could also opt to create a "missing values" bucket.
This can cost some query performance.
We can always check the query performance with the inspect dialog looking at "Requests" and the "Response" tab.

![Visualization Query Performance](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_performance.png){:class="img-centered"}

The value reported as `"took": 1787` indicates a latency of 1787ms.
We can always use this feature to analyze the impact of different aggregations and metrics configurations on the query runtime.
This can be very helpful for complicated visualizations.

One of the main benefits of the standard visualizations is, that they allow to create global filters.
In our table, the first column allows filtering on a specific namespace (plus symbol) or excluding it (minus symbol).

![Visualization Filter Creation](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/table_metric_selector.png){:class="img-centered"}

Clicking on either icon will create the corresponding filter and reload the entire dashboard.
This makes the data tables good candidates to provide selectors by different dimensions.
The metrics can help users making decisions what data to focus on.
In our example, we provide a general overview on the size and variety of a namespace.

#### A Line Chart for Timelines

We will now push the standard line chart to its limits by trying to recreate the earlier TSVB visualization of the Pod CPU time.
Actually, we will go a little beyond it and attempt to visualize the container CPU time for each pod.

For our [data table](#a-data-table-for-k8s-namespaces) we created in the last subsection, we used an index pattern.
This time, we will create a proper saved search in the Discovery plugin.
We need to filter our OpenTelemetry metrics index by metric name `container.cpu.time`.

![Saved Search for container cpu time](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/discover_k8s-container-cpu-time.png){:class="img-centered"}

This screenshot is actually superfluous to show both possibilities: Either use DQL in the search input or create the equivalent filter.
Our example contains an additional filter for the namespace, to show the possibility to add additional filters.
We save our search, so that we can reference it, when we create our line chart.
The major benefit of this approach is, that we can refine our saved search during the creation of our visualization.

![New Line Chart Visualization](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_create_line-chart.png){:class="img-centered"}

![Select Saved Search](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/visualize_select-saved-search.png){:class="img-centered"}

We now need to provide several configurations.
Let us start with the x-axis, which needs to be a date histogram.
This divides the time interval selected on the upper right of the screen into time buckets.

![Date Histogram](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_date-histogram.png){:class="img-centered"}

The default time field for OpenTelemetry metrics is `time`.
The minimum bucket length needs to be at least the reporting interval, to avoid empty buckets.
The actual length will be selected from the chosen interval.

We can now configure our y-axis.

![Calculating the CPU Time Slice](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_y-axis.png){:class="img-centered"}

We want to use the derivative of each interval.
Since this is a parent aggregation, we need a time bucket aggregation as well.
Here we use the maximum, since the metric is a monotonically increasing counter.

In comparison with TSVB, we have no control over the derivation scale.
That means the derivative will be dependent on the interval length.
This is why, we did not give a custom label on the x-axis, so that it will print the interval length.
This is the first limitation of the standard line chart, we encounter.

Note, that configuring the date histogram on the x-axis is required for using the derivative aggregation.

As a finishing touch, we will now split our data series by K8s pod name and K8s container name.
This is something, we could not do in TSVB, which only supported grouping by one field.
We start with the pod name.

![Split by Pod Name](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_split_pod-name.png){:class="img-centered"}

Note, that for ordering we are again restricted to the max aggregation.
We cannot sort by the derivative well, just like with TSVB.
After the pod, we split again by container name.

![Split by Container Name](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_split_container-name.png){:class="img-centered"}

When you add the "split series" configuration, OpenSearch Dashboards will complain, that the Date Histogram needs to be the last visualization.
You can rearrange the bucket aggregations by drag and drop on the "=" icon.

With all that configuration done, we hit the Update button to inspect the graph.

![CPU Time by Container and Pod](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_chart.png){:class="img-centered"}

We get a line of the cpu time used by time interval for each container.
The labels consist of the pod name first and the container name second.
We can control this ordering by the ordering of our aggregations.
Similar to TSVB, we have a problem to distinguish long labels in the legend.
This is no better, if we bring the legend to the bottom with the panel options.

If we mouse-over a data point, we get additional information.

![Line Chart Point Info](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_data-point_info.png){:class="img-centered"}

We can see the timestamp and cpu time used.
The labels for pod and container names are presented, but hard to recognize.
This is caused by the double aggregation.
If we used just pod names, this would be much better.
An alternative approach would be to use "split series" for container names and "split chart" for pod names.

Clicking on a data point brings up a filter creation dialog.

![Line Chart Point Filter](/assets/media/blog-images/2024-05-07-opentelemetry-metrics-visualization/line_data-point_filters.png){:class="img-centered"}

We can use these options to slice our data.
These filters are applied across the entire dashboard.

We have pushed our timeline visualization to the limits of the standard charts.
This gives us a good understanding of its capabilities.
It has its benefits for creating filters and grouping by multiple attributes.
But TSVB has easier options to format data.

### Outlook: Vega

We have explored, how to set up present OpenTelemetry metrics using TSVB and the standard visualizations.
Both approaches allowed the creation of useful charts providing powerful insights.
But we also encounter quite some limitations.
To overcome those limits, OpenSearch Dashboards offers Vega as another powerful visualization approach.
Vega lets us use any OpenSearch query and provides a grammar to transform and visualize the data with [D3.js](https://d3js.org/).
It allows to access global filters and the time interval selector to create well-integrated exploration and analysis journeys in OpenSearch.

Of course, this has a steeper learning curve.
Explaining Vega visualizations would warrant its own blog post.
There is a short introduction in the [OpenSearch Catalog](https://github.com/opensearch-project/opensearch-catalog/blob/main/visualizations/vega-visualizations.md).
Be sure to check out the Vega visualizations in that catalog to get further inspiration.