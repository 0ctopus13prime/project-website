---
layout: post
title:  "Optimizing OpenSearch with FP16 Quantization: Enhancing Memory Efficiency and Cost-Effectiveness"
authors:
  - naveen
  - vamshin
  - tal
date: 2024-06-19 00:00:00 -0700
categories:
  - technical-posts
meta_keywords: FP16 quantization, OpenSearch k-NN plugin, memory optimization, cost-effectiveness
meta_description: Learn how FP16 Quantization in OpenSearch helps to reduce memory requirements upto 50% with a very minimal loss in quality.
has_math: true
---

The rise of large language models (LLMs) and generative AI has ushered in a new era of natural language processing capabilities. Vector databases have emerged as a crucial 
component in this ecosystem, acting as external databases that can efficiently index, store, and retrieve embeddings generated by LLMs. However, as the scale and complexity 
of LLMs continue to grow, the workloads on vector databases have also increased significantly. Ingesting and querying billions of vectors can strain computational resources, 
leading to higher memory requirements and increased operational costs. This feature enables you to store vector embeddings with lower precision resulting in cost reduction 
by reducing the memory consumption.

## How does OpenSearch k-NN plugin solves this problem ?

When you index vectors on [OpenSearch 2.13](https://github.com/opensearch-project/opensearch-build/blob/main/release-notes/opensearch-release-notes-2.13.0.md) or later versions, you can configure your k-NN index to apply a technique called scalar quantization. This technique converts each dimension 
of the vector from 32-bit floating-point (fp32) to 16-bit floating-point (fp16) representation. The conversion reduces the precision and memory requirements of your vector by half, 
resulting in significant memory savings.

k-NN plugin integrated Faiss Scalar Quantizer and added support for this Faiss Scalar Quantization FP16 (SQfp16) feature which helps to quantize your 32 bit float vectors into 
fp16 vectors by providing memory savings up to 50% with a very minimal loss of recall(refer benchmarks in later sections). When used with [SIMD optimization](https://opensearch.org/docs/latest/search-plugins/knn/knn-index#simd-optimization-for-the-faiss-engine), 
SQfp16 quantization can also significantly reduce search latencies and improve indexing throughput.

## How to use faiss scalar quantization fp16 ?

To use Faiss scalar quantization, set the k-NN vector field's `method.parameters.encoder.name` to `sq` when creating a k-NN index:

```json
PUT /test-index
{
  "settings": {
    "index": {
      "knn": true
    }
  },
  "mappings": {
    "properties": {
      "my_vector1": {
        "type": "knn_vector",
        "dimension": 8,
        "method": {
          "name": "hnsw",
          "engine": "faiss",
          "space_type": "l2",
          "parameters": {
            "encoder": {
              "name": "sq",
              "parameters": {
                "type": "fp16",
                "clip": true
              }
            },
            "ef_construction": 256,
            "m": 8
          }
        }
      }
    }
  }
}
```

Refer to [this k-NN documentation](https://opensearch.org/docs/latest/search-plugins/knn/knn-index/#sq-parameters) to know more details about the SQ parameters.

The `fp16` encoder converts 32-bit vectors into their 16-bit counterparts. For this encoder type, the vector values must be in the **[-65504.0, 65504.0]** range.

To define how to handle out-of-range values, the above index mapping request specifies the `clip` parameter.

* By default, this `clip` parameter is `false`, and any vectors containing out-of-range values are rejected.
* When the `clip` is set to `true`, out-of-range vector values are rounded up or down so that they are in the supported range. For example, if the original 32-bit vector is 
`[65510.82, -65504.1]`, the vector will be indexed as a 16-bit vector `[65504.0, -65504.0]`.

**Note** - We recommend setting `clip` to `true` only if very few elements lie outside of the supported range. Rounding the values may cause a drop in recall.

During ingestion, make sure each dimension of the vector is in the supported range ([-65504.0, 65504.0]):
```json
PUT test-index/_doc/1
{
  "my_vector1": [-65504.0, 65503.845, 55.82, -65300.456, 34.67, -1278.23, 90.62, 8.36]
}
```

During querying, there is no range limitation for the query vector:
```json
GET test-index/_search
{
  "size": 2,
  "query": {
    "knn": {
      "my_vector1": {
        "vector": [265436.876, -120906.256, 99.84, 89.45, 100000.45, 9.23, -70.17, 6.93],
        "k": 2
      }
    }
  }
}
```

## Memory estimation

### HNSW memory estimation with fp16

The memory required for HNSW is estimated to be `1.1 * (2 * dimension + 8 * M)` bytes/vector.

As an example, assume that you have 1 million vectors with a dimension of 256 and M of 16. The memory requirement can be estimated as follows:

`1.1 * (2 * 256 + 8 * 16) * 1,000,000 ~= 0.656 GB`

To know the memory estimation for IVF with fp16, refer to [this documentation](https://opensearch.org/docs/latest/search-plugins/knn/knn-vector-quantization/#memory-estimation-1).

## Benchmarking Results

We ran benchmarking tests on some of the popular and trending datasets using our [opensearch-benchmark](https://github.com/opensearch-project/opensearch-benchmark-workloads/tree/main/vectorsearch) tool 
to compare the indexing, search performance, and the quality of search results of Faiss SQfp16 against float vectors (using Faiss without any encoding). All these tests were performed with [SIMD](https://opensearch.org/docs/latest/search-plugins/knn/knn-index/#simd-optimization-for-the-faiss-engine) 
enabled (on x86 architecture with AVX2 optimization).

**Note** - Without SIMD optimization(AVX2 or NEON) or with AVX2 disabled(on x86 architecture), the quantization process adds an additional overhead which leads to an increase in latencies. 
These are the [processors](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX2) that supports AVX2. In AWS, for x86 architecture all the community AMIs with [HVM](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html) supports AVX2 optimization.

### Benchmarking results using small workloads

All these tests were ran on a single node cluster without any replica using these datasets mentioned below.

#### Configuration
<table>
    <tr>
        <td colspan="1"><b>m</b></td>
        <td colspan="1"><b>ef_construction</b></td>
        <td colspan="1"><b>ef_search</b></td>
        <td colspan="1"><b>replica</b></td>
    </tr>
    <tr>
        <td>16</td>
        <td>100</td>
        <td>100</td>
        <td>0</td>
    </tr>
</table>


<table>
    <tr>
        <td colspan="1"><b>Dataset ID</b></td>
        <td colspan="1"><b>Dataset</b></td>
        <td colspan="1"><b>Dimension of vector</b></td>
        <td colspan="1"><b>Data size</b></td>
        <td colspan="1"><b>Number of queries</b></td>
        <td colspan="1"><b>Training data range</b></td>
        <td colspan="1"><b>Query data range</b></td>
        <td colspan="1"><b>Space type</b></td>
        <td colspan="1"><b>Primary shards</b></td>
        <td colspan="1"><b>Indexing clients</b></td>
    </tr>
    <tr>
        <td><b>Dataset 1</b></td>
        <td><b>gist-960-euclidean</b></td>
        <td>960</td>
        <td>1,000,000</td>
        <td>1,000</td>
        <td>[ 0.0, 1.48 ]</td>
        <td>[ 0.0, 0.729 ]</td>
        <td>L2</td>
        <td>8</td>
        <td>16</td>
    </tr>
    <tr>
        <td><b>Dataset 2</b></td>
        <td><b>mnist-784-euclidean</b></td>
        <td>784</td>
        <td>60,000</td>
        <td>10,000</td>
        <td>[ 0.0, 255.0 ]</td>
        <td>[ 0.0, 255.0 ]</td>
        <td>L2</td>
        <td>1</td>
        <td>2</td>
    </tr>
    <tr>
        <td><b>Dataset 3</b></td>
        <td><b>cohere-wiki-simple-embeddings-768</b></td>
        <td>768</td>
        <td>475,858</td>
        <td>10,000</td>
        <td>[ -4.1561704, 5.5478516 ]</td>
        <td>[ -4.065383, 5.4902344 ]</td>
        <td>L2</td>
        <td>4</td>
        <td>8</td>
    </tr>
    <tr>
        <td><b>Dataset 4</b></td>
        <td><b>cohere-ip-1m</b></td>
        <td>768</td>
        <td>1,000,000</td>
        <td>10,000</td>
        <td>[ -4.1073565, 5.504557 ]</td>
        <td>[ -4.109505, 5.4809895 ]</td>
        <td>InnerProduct</td>
        <td>8</td>
        <td>16</td>
    </tr>
    <tr>
        <td><b>Dataset 5</b></td>
        <td><b>sift-128-euclidean</b></td>
        <td>128</td>
        <td>1,000,000</td>
        <td>10,000</td>
        <td>[ 0.0, 218.0 ]</td>
        <td>[ 0.0, 184.0 ]</td>
        <td>L2</td>
        <td>8</td>
        <td>16</td>
    </tr>
</table>

#### Recall and memory results

<table>
    <tr>
        <td colspan="1"><b>Dataset ID</b></td>
        <td colspan="1"><b>Faiss hnsw recall@100</b></td>
        <td colspan="1"><b>Faiss hnsw-sqfp16 recall@100</b></td>
        <td colspan="1"><b>Faiss hnsw memory estimate (gb)</b></td>
        <td colspan="1"><b>Faiss hnsw-sqfp16 memory estimate (gb)</b></td>
        <td colspan="1"><b>Faiss hnsw memory usage (gb)</b></td>
        <td colspan="1"><b>Faiss hnsw-sqfp16 memory usage (gb)</b></td>
        <td colspan="1"><b>% reduction in memory</b></td>
    </tr>
    <tr>
        <td><b>Dataset 1</b></td>
        <td>0.9071</td>
        <td>0.9072</td>
        <td>4.07</td>
        <td>2.10</td>
        <td>3.72</td>
        <td>1.93</td>
        <td>48.12</td>
    </tr>
    <tr>
        <td><b>Dataset 2</b></td>
        <td>0.9889</td>
        <td>0.9889</td>
        <td>0.20</td>
        <td>0.10</td>
        <td>0.18</td>
        <td>0.10</td>
        <td>44.44</td>
    </tr>
    <tr>
        <td><b>Dataset 3</b></td>
        <td>0.9456</td>
        <td>0.9450</td>
        <td>1.56</td>
        <td>0.81</td>
        <td>1.43</td>
        <td>0.75</td>
        <td>47.55</td>
    </tr>
    <tr>
        <td><b>Dataset 4</b></td>
        <td>0.9429</td>
        <td>0.9422</td>
        <td>3.28</td>
        <td>1.70</td>
        <td>3.00</td>
        <td>1.57</td>
        <td>47.67</td>
    </tr>
    <tr>
        <td><b>Dataset 5</b></td>
        <td>0.9925</td>
        <td>0.9925</td>
        <td>0.66</td>
        <td>0.39</td>
        <td>0.62</td>
        <td>0.38</td>
        <td>38.71</td>
    </tr>
</table>

#### Indexing and Query results

<table>
    <tr>
        <td colspan="1"><b>Dataset ID</b></td>
        <td colspan="1"><b>Faiss hnsw mean indexing throughput (docs/sec)</b></td>
        <td colspan="1"><b>Faiss hnsw-sqfp16 mean indexing throughput (docs/sec)</b></td>
        <td colspan="1"><b>Faiss hnsw p90 (ms)</b></td>
        <td colspan="1"><b>Faiss hnsw-sqfp16 p90 (ms)</b></td>
        <td colspan="1"><b>Faiss hnsw p99 (ms)</b></td>
        <td colspan="1"><b>Faiss hnsw-sqfp16 p99 (ms)</b></td>
    </tr>
    <tr>
        <td><b>Dataset 1</b></td>
        <td>4681</td>
        <td>4696</td>
        <td>4.97</td>
        <td>5.08</td>
        <td>5.54</td>
        <td>5.50</td>
    </tr>
    <tr>
        <td><b>Dataset 2</b></td>
        <td>4271</td>
        <td>4580</td>
        <td>2.01</td>
        <td>2.06</td>
        <td>2.16</td>
        <td>2.21</td>
    </tr>
    <tr>
        <td><b>Dataset 3</b></td>
        <td>4690</td>
        <td>4698</td>
        <td>3.35</td>
        <td>3.33</td>
        <td>3.58</td>
        <td>3.57</td>
    </tr>
    <tr>
        <td><b>Dataset 4</b></td>
        <td>6044</td>
        <td>6129</td>
        <td>4.61</td>
        <td>4.81</td>
        <td>5.16</td>
        <td>5.37</td>
    </tr>
    <tr>
        <td><b>Dataset 5</b></td>
        <td>115499</td>
        <td>102060</td>
        <td>2.73</td>
        <td>2.68</td>
        <td>2.96</td>
        <td>2.89</td>
    </tr>
</table>

#### Analysis

Comparing the benchmarking results, you can see that:

* The recall obtained using Faiss HNSW SQfp16 matches that of Faiss HNSW (with negligible difference).
* Using SQfp16, there is a significant reduction in memory usage of up to **48%**, with a slight reduction in disk usage. Based on the results, we can see that a larger dimension of the vector provides better memory reduction.
* The performance metrics using SQfp16 are on par with those of fp32 vectors.

### Benchmarking results using large workloads

These tests were ran using some large scale workload, Laion 100M dataset with 768 dimensions to show the comparison between the performance metrics and memory savings obtained for Faiss HNSW SQfp16 against Faiss HNSW.

#### Configuration

<table>
    <tr>
        <td colspan="1"><b></b></td>
        <td colspan="1"><b>Faiss HNSW SQfp16</b></td>
        <td colspan="1"><b>Faiss HNSW</b></td>
    </tr>
    <tr>
        <td><b>OpenSearch version</b></td>
        <td>2.13</td>
        <td>2.13</td>
    </tr>
    <tr>
        <td><b>Engine</b></td>
        <td>faiss</td>
        <td>faiss</td>
    </tr>
    <tr>
        <td><b>Dimension of vector</b></td>
        <td>768</td>
        <td>768</td>
    </tr>
    <tr>
        <td><b>Ingest vectors</b></td>
        <td>100M</td>
        <td>100M</td>
    </tr>
    <tr>
        <td><b>Test vectors</b></td>
        <td>1K</td>
        <td>1K</td>
    </tr>
    <tr>
        <td><b>Primary shards</b></td>
        <td>36</td>
        <td>36</td>
    </tr>
    <tr>
        <td><b>Replica shards</b></td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <td><b>Data nodes</b></td>
        <td>4</td>
        <td>8</td>
    </tr>
    <tr>
        <td><b>Data node instance type</b></td>
        <td>r5.4xlarge</td>
        <td>r5.4xlarge</td>
    </tr>
    <tr>
        <td><b>Master nodes</b></td>
        <td>3</td>
        <td>3</td>
    </tr>
    <tr>
        <td><b>Master node instance type</b></td>
        <td>c5.xlarge</td>
        <td>c5.xlarge</td>
    </tr>
    <tr>
        <td><b>Indexing clients</b></td>
        <td>9</td>
        <td>9</td>
    </tr>
    <tr>
        <td><b>Query clients</b></td>
        <td>1</td>
        <td>1</td>
    </tr>
    <tr>
        <td><b>Forcemerge clients</b></td>
        <td>1</td>
        <td>1</td>
    </tr>
    <tr>
        <td><b>Client instance</b></td>
        <td>r5.16xlarge</td>
        <td>r5.16xlarge</td>
    </tr>
</table>

<table>
    <tr>
        <td><b>Config ID</b></td>
        <td><b>Optimization strategy</b></td>
        <td><b>m</b></td>
        <td><b>ef_construction</b></td>
        <td><b>ef_search</b></td>
    </tr>
    <tr>
        <td><b>hnsw1</b></td>
        <td>Default Configuration</td>
        <td>16</td>
        <td>100</td>
        <td>100</td>
    </tr>
    <tr>
        <td><b>hnsw2</b></td>
        <td>Balance between latency, memory, and recall</td>
        <td>16</td>
        <td>128</td>
        <td>128</td>
    </tr>
    <tr>
        <td><b>hnsw3</b></td>
        <td>Optimize for recall</td>
        <td>16</td>
        <td>256</td>
        <td>256</td>
    </tr>
</table>

The Faiss HNSW SQfp16 require 4 data nodes which is half the number of data nodes required for Faiss HNSW (8) which proves that SQfp16 reduces the memory requirements by 50%. 
To know more details about the calculation, refer to appendix section.

#### Recall and memory results

<table>
    <tr>
        <td><b>Experiment ID</b></td>
        <td><b>hnsw-recall@1000</b></td>
        <td><b>hnsw-sqfp16-recall@1000</b></td>
        <td><b>hnsw memory usage (gb)</b></td>
        <td><b>hnsw-sqfp16 memory usage (gb)</b></td>
        <td><b>% reduction in memory</b></td>
    </tr>
    <tr>
        <td><b>hnsw1</b></td>
        <td>0.94</td>
        <td>0.94</td>
        <td>300.28</td>
        <td>157.23</td>
        <td>47.64</td>
    </tr>
    <tr>
        <td><b>hnsw2</b></td>
        <td>0.96</td>
        <td>0.96</td>
        <td>300.28</td>
        <td>157.23</td>
        <td>47.64</td>
    </tr>
    <tr>
        <td><b>hnsw3</b></td>
        <td>0.98</td>
        <td>0.98</td>
        <td>300.28</td>
        <td>157.23</td>
        <td>47.64</td>
    </tr>
</table>

#### Indexing and Query results

<table>
    <tr>
        <td><b>Experiment ID</b></td>
        <td><b>hnsw mean indexing throughput (docs/sec)</b></td>
        <td><b>hnsw-sqfp16 mean indexing throughput (docs/sec)</b></td>
        <td><b>hnsw p90 (ms)</b></td>
        <td><b>hnsw-sqfp16 p90 (ms)</b></td>
         <td><b>hnsw p99 (ms)</b></td>
        <td><b>hnsw-sqfp16 p99 (ms)</b></td>
    </tr>
    <tr>
        <td><b>hnsw1</b></td>
        <td>7544</td>
        <td>7657</td>
        <td>14.02</td>
        <td>16.99</td>
        <td>19.18</td>
        <td>20.83</td></td>
    </tr>
    <tr>
        <td><b>hnsw2</b></td>
        <td>7063</td>
        <td>7219</td>
        <td>14.21</td>
        <td>17.44</td>
        <td>18.86</td>
        <td>21.80</td>
    </tr>
    <tr>
        <td><b>hnsw3</b></td>
        <td>6004</td>
        <td>5848</td>
        <td>16.14</td>
        <td>20.85</td>
        <td>17.65</td>
        <td>24.73</td>
    </tr>
</table>

#### Analysis

* For k=1000, the recall is identical for both Faiss HNSW and Faiss HNSW with SQfp16.
* The memory requirements (number of data nodes) are reduced by half, and based on the [k-NN stats API metrics](https://opensearch.org/docs/latest/search-plugins/knn/api/#stats), the memory usage has been reduced by 47.64% using SQfp16.
* In most instances, SQfp16 demonstrated better indexing throughput compared to fp32 vectors.

## Conclusion

Faiss Scalar Quantization FP16 (Float16) is a powerful technique that enables significant memory savings while maintaining high recall performance comparable to full-precision vectors. 
By quantizing vectors to 16-bit floating-point representation, it can reduce memory requirements by up to 50%. Additionally, when combined with SIMD (Single Instruction Multiple Data) 
optimization, Faiss Scalar Quantization FP16 offers enhanced indexing throughput and lower search latencies, resulting in improved overall performance. This technique strikes an excellent 
balance between memory efficiency and accuracy, making it a valuable tool for large-scale similarity search applications.

## Future Scope

To achieve even greater memory efficiency, we plan to introduce int8 quantization support using [Faiss Scalar Quantizer](https://github.com/opensearch-project/k-NN/issues/1723) and [Lucene Scalar Quantizer](https://github.com/opensearch-project/k-NN/issues/1277). 
This advanced technique will enable a remarkable 75% reduction in memory requirements, or 4x compression, compared to full-precision vectors, while maintaining high recall performance. 
The quantizers will accept FP32 vectors as input, perform online training, and quantize the data into byte-sized vectors, eliminating the need for external quantization or extra training steps.

Furthermore, we aim to release Binary vector support, enabling an unprecedented 32X compression rate. This groundbreaking approach will further reduce memory consumption. 
By combining these cutting-edge quantization techniques, we will provide a comprehensive solution for efficient similarity search, balancing memory optimization and 
accurate retrieval.

Our commitment to continuous innovation ensures that our users can leverage state-of-the-art technologies to tackle large-scale similarity search challenges while minimizing resource 
requirements and maximizing cost-effectiveness.

## Appendix

### Memory estimation and data nodes calculation

Here is the memory estimation and data nodes calculation for the 100M, 768 dimension large workload benchmarking test.
```
// Faiss HNSW SQfp16 Memory Estimation
1.1 * (2 * dimension + 8 * M) * num_of_vectors * (1 + num_of_replicas) bytes

Let m = 16 and num_replicas = 0

1.1 * (2 * 768 + 8 * 16) * 100000000 * (1 + 0) = 170.47 gb = 171 gb

Instance r5.4xlarge has a memory of 128 gb in which 32 gb is used for JVM. 
Let us assume circuit breaker limit is 0.5

Total available memory = (data node instance memory - jvm memory) * circuit breaker limit
Total available memory = (128 - 32 ) * 0.5 = 48gb

Number of Data nodes -> 171/48 = 3.56 = 4
```

```
// Faiss HNSW Memory Estimation
1.1 * (4 * dimension + 8 * M) * num_of_vectors * (1 + num_of_replicas) bytes

Let m = 16 and num_replicas = 0

1.1 * (4 * 768 + 8 * 16) * 100000000 * (1 + 0) = 327.83 gb = 328 gb

Instance r5.4xlarge has a memory of 128 gb in which 32 gb is used for JVM. 
Let us assume circuit breaker limit is 0.5

Total available memory = (data node instance memory - jvm memory) * circuit breaker limit
Total available memory = (128 - 32 ) * 0.5 = 48gb

Number of Data nodes -> 328/48 = 6.83 = 7 + 1(for stability) = 8
```

## References

* https://github.com/erikbern/ann-benchmarks?tab=readme-ov-file#data-sets
* https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings
* https://dbyiw3u3rf9yr.cloudfront.net/corpora/vectorsearch/cohere-wikipedia-22-12-en-embeddings/documents-1m.hdf5.bz2
* https://laion.ai/about/
* Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J. (2022). LAION-5B: An open large-scale dataset for training next generation image-text models. arXiv (Cornell University). https://doi.org/10.48550/arxiv.2210.08402
* Douze, Matthijs, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar'e, Maria Lomeli, Lucas Hosseini and Herv'e J'egou. “The Faiss library.” ArXiv abs/2401.08281 (2024): n. pag.
