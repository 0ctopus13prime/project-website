---
layout: post
title:  "Testing Automation for OpenSearch Releases"
authors: 
  - setiah
date: 2021-10-01 01:01:01 -0700
categories: 
  - technical-post
twittercard:
  description: "This post details the automated testing process for OpenSearch 1.1 and subsequent releases."
---

# Testing Automation for OpenSearch Releases

OpenSearch releases many [distributions](https://opensearch.org/downloads.html) across multiple platforms as part of a new version release. These distributions are of two types - the default distribution that includes all plugins, and the min distribution without any plugins. These release distributions go through a rigorous testing process across multiple teams, before they are signed off as “release ready”. It includes unit testing, integration testing to validate the behavioral integrity, backward compatibility testing to ensure upgrade compatibility with previous versions, and stress testing to validate the performance characteristics. Once these distributions are successfully tested, they are marked ready for the release.

The rigorous testing process provides good confidence in the quality of the release. However, so far it has been manual and non-standardized across plugins. Each plugin team validated their component by running tests on the distribution and provided their sign-off manually. With dozens of OpenSearch plugins released as part of default distribution, the turn around time for testing was high. Also, lack of a continuous integration and testing process, lead to new bug discoveries at the time of release, which further added to release times.

To overcome these problems, we built an automated testing framework for OpenSearch releases that simplifies and standardizes the testing process across all components of a release distribution.

The way it works is once a new bundle is ready, the [build-workflow](https://github.com/opensearch-project/opensearch-build/blob/1.1.0/bundle-workflow/README.md#build-from-source) (explained in this [blog](https://opensearch.org/blog/technical-post/2021/10/building-opensearch-1-1-distributions/)), kicks off the `test-orchestrator-pipeline` with [input parameters](https://github.com/opensearch-project/opensearch-build/blob/56f4092bdac8ee676ac9610a594d6657149e06be/jenkins/test/orchestrator/Jenkinsfile#L7-L9) that uniquely identify the bundle in S3. The test orchestrator-pipeline, is a Jenkins pipeline based on this [JenkinsFile](https://github.com/opensearch-project/opensearch-build/blob/main/bundle-workflow/jenkins_workflow/test/orchestrator/Jenkinsfile). It orchestrates the test workflow, which consists of three test suites - `integ-test` (integration testing)`, bwc-test` (backward compatibility testing), `perf-test`(performance testing), to run in parallel. Each of these test suites is a [jenkins workflow](https://github.com/opensearch-project/opensearch-build/blob/main/bundle-workflow/jenkins_workflow/test/testsuite/Jenkinsfile) that executes the respective test type.

Like build-workflow, these test workflows are manifest-based workflows. `integ-test` suite reads bundle manifest file to identify the type and components of a bundle under test. It pulls all maven and build dependencies for running integration tests on the bundle from s3 (these dependencies are built as part of build-workflow and re-used for testing). After pulling the dependencies, it runs integration tests for each component in the distribution, based on the component test config defined in the [test-manifest file](https://github.com/opensearch-project/opensearch-build/blob/main/bundle-workflow/src/test_workflow/config/test_manifest.yml). It spins a new dedicated local cluster to test each [test config](https://github.com/opensearch-project/opensearch-build/blob/3d332e568de32ea6c26b63eeec2590c01d159e35/bundle-workflow/src/test_workflow/config/test_manifest.yml#L6-L8) and tears down the cluster after the test run completes. The test and the cluster logs are published to S3 after the test workflow completes. `bwc-test` suite runs similar to `integ-test` suite, for backward compatibility tests. Currently, it only supports backward compatibility tests for OpenSearch and anomaly-detection plugin, but, there’s ongoing effort to add more plugins. `perf-test` suite runs performance testing with rally tracks on a dedicated external cluster. This piece is currently in development. Once all test suites complete, the notifications job sends out notifications to all subscribed channels. Figure 1 illustrates how different components of the test workflow interact with each other. Figure 2 shows a sample test report generated by the integration testing workflow on a release candidate build.

**Figure 1**: Automated test workflow explained

![Figure 1: Automated test workflow]({{ site.baseurl }}/assets/media/blog-images/assets/media/blog-images/2021-10-01-automated-testing-for-opensearch-releases/figure1.png){: .img-fluid }

**Figure 2**: A sample test reported generated by `integ-test` workflow. `with-security` config denotes the test run with security plugin enabled, `without-security` config denotes the test run without security plugin enabled.

![Figure 2: Test report]({{ site.baseurl }}/assets/media/blog-images/assets/media/blog-images/2021-10-01-automated-testing-for-opensearch-releases/figure2.png){: .img-fluid }

This testing automation helps make the release process faster by providing a quick feedback loop to surface issues sooner. It also standardizes the testing process and enforces strict quality controls across all components.

The code is entirely [open source](https://github.com/opensearch-project/opensearch-build) and development work is being tracked on this [project board](https://github.com/opensearch-project/opensearch-build/projects/3). Currently, this `test workflow` cannot be executed locally, as it requires the build artifacts to be available in S3. There is an [open issue](https://github.com/opensearch-project/opensearch-build/issues/629) to extend this support to read artifacts from local file system, which would enable developers to build and test everything locally. We also plan to extend backward compatibility test suite and performance test suite to support more components and configurations. We welcome your comments and contributions to make this system better and more useful for everyone.
